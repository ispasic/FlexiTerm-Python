{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- FlexiTerm: multi-word term recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- dependencies ---\n",
    "\n",
    "import csv\n",
    "import jellyfish\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pathlib import Path\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- database connection ---\n",
    "\n",
    "schema = \"./config/schema.sql\"           # --- read database schema\n",
    "\n",
    "try: \n",
    "    with open(Path(schema),'r') as file:\n",
    "        sql_script = file.read()\n",
    "        file.close()\n",
    "        print(sql_script[0:100] + '...') # --- preview schema\n",
    "except:\n",
    "    print(\"ERROR: Schema file \" + schema + \" not found. Unable to create the tables.\\n\")\n",
    "    quit()\n",
    "\n",
    "# --- database connection\n",
    "con = sqlite3.connect('flexiterm.sqlite')\n",
    "\n",
    "# --- cursor (statement) objects to execute SQL queries\n",
    "cur1 = con.cursor()\n",
    "cur2 = con.cursor()\n",
    "cur3 = con.cursor()\n",
    "\n",
    "# --- create database tables\n",
    "cur1.executescript(sql_script)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- default settings ---\n",
    "\n",
    "default = {\n",
    "   \"pattern\"  : \"(((((NN|JJ) )*NN) IN (((NN|JJ) )*NN))|((NN|JJ )*NN POS (NN|JJ )*NN))|(((NN|JJ) )+NN( CD)?)\",\n",
    "   \"stoplist\" : \"./config/stoplist.txt\",\n",
    "   \"Smin\"     : 0.962,\n",
    "   \"Amin\"     : 5,\n",
    "   \"Fmin\"     : 2,\n",
    "   \"Cmin\"     : 1,\n",
    "   \"acronyms\" : \"explicit\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load settings ---\n",
    "\n",
    "settings_file = \"./config/settings.json\"\n",
    "\n",
    "try: \n",
    "    with open(Path(settings_file),\"r\") as file:\n",
    "        \n",
    "        settings = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        if \"pattern\" in settings: \n",
    "            pattern = settings[\"pattern\"]\n",
    "            try: re.compile(pattern)\n",
    "            except re.error: \n",
    "                print(\"WARNING: Invalid POS pattern: \" + pattern)\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                pattern = default[\"pattern\"]\n",
    "\n",
    "        if \"stoplist\" in settings: \n",
    "            stoplist = settings[\"stoplist\"]\n",
    "            if not os.path.isfile(stoplist):\n",
    "                print(\"WARNING: Stoplist file \" + stoplist + \" not found.\")\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                stoplist = default[\"stoplist\"]\n",
    "\n",
    "        if \"Smin\" in settings:\n",
    "            Smin = settings[\"Smin\"]\n",
    "            if not (0 < Smin and Smin < 1):\n",
    "                print(\"WARNING: Invalid token similarity threshold:\", Smin);\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                Smin = default[\"Smin\"]\n",
    "\n",
    "        if \"Amin\" in settings: \n",
    "            Amin = settings[\"Amin\"]\n",
    "            if type(Amin) != int:\n",
    "                print(\"WARNING: Invalid acronym frequency threshold:\", Amin);\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                Amin = default[\"Amin\"]\n",
    "        \n",
    "        if \"Fmin\" in settings:\n",
    "            Fmin = settings[\"Fmin\"]\n",
    "            if type(Fmin) != int:\n",
    "                print(\"WARNING: Invalid term frequency threshold:\", Fmin);\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                Fmin = default[\"Fmin\"]\n",
    "            \n",
    "        if \"Cmin\" in settings:\n",
    "            Cmin = settings[\"Cmin\"]\n",
    "            if Cmin < 0.7:\n",
    "                print(\"WARNING: Invalid token C-value threshold:\", Cmin);\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                Cmin = default[\"Cmin\"]\n",
    "                \n",
    "        if \"acronyms\" in settings:\n",
    "            acronyms = settings[\"acronyms\"]\n",
    "            if acronyms not in [\"explicit\", \"implicit\"]:\n",
    "                print(\"WARNING: Invalid acronyms value:\", acronyms);\n",
    "                print(\"         Using the default instead.\\n\")\n",
    "                acronyms = default[\"acronyms\"]\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Settings file \" + settings_file + \" not found. Using the default values instead.\\n\")\n",
    "    \n",
    "    pattern = default[\"pattern\"]\n",
    "    stoplist = default[\"stoplist\"]\n",
    "    Smin = default[\"Smin\"]\n",
    "    Amin = default[\"Amin\"]\n",
    "    Fmin = default[\"Fmin\"]\n",
    "    Cmin = default[\"Cmin\"]\n",
    "    acronyms = default[\"acronyms\"]\n",
    "\n",
    "print(\"--- Settings ---\")\n",
    "print(\"* pattern  :\", pattern)\n",
    "print(\"* stoplist :\", stoplist)\n",
    "print(\"* Smin     :\", Smin)\n",
    "print(\"* Amin     :\", Amin)\n",
    "print(\"* Fmin     :\", Fmin)\n",
    "print(\"* Cmin     :\", Cmin)\n",
    "print(\"* acronyms :\", acronyms)\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load stoplist ---\n",
    "\n",
    "print(\"Loading stoplist from \" + stoplist + \"...\");\n",
    "\n",
    "try: \n",
    "    # --- read a CSV file\n",
    "    table = open(Path(stoplist),'r')\n",
    "    rows = csv.reader(table)\n",
    "    \n",
    "    # --- insert rows from the CSV file\n",
    "    cur1.executemany(\"INSERT INTO stopword (word) VALUES (?);\", rows)\n",
    "    con.commit()\n",
    "\n",
    "    table.close()\n",
    "\n",
    "except sqlite3.Error as error: print(error)\n",
    "    \n",
    "file = open(Path(stoplist), 'r')\n",
    "stopwords = file.read().split('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load language model from spacy ---\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentencizer = nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- delete previous output files if any\n",
    "\n",
    "folder = \"./out\"\n",
    "filename = [\"annotations.json\", \n",
    "            \"concordances.html\", \n",
    "            \"corpus.html\", \n",
    "            \"terminology.html\", \n",
    "            \"terminology.csv\"]\n",
    "\n",
    "for name in filename:\n",
    "    file_path = os.path.join(folder, name)\n",
    "    if os.path.exists(file_path): os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- load & preprocess input documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "timer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fix potential tagging issues\n",
    "def pretagging(txt):\n",
    "    \n",
    "    unit = [\"meter\",\n",
    "          \"metre\",\n",
    "          \"mile\",\n",
    "          \"centi\",\n",
    "          \"milli\",\n",
    "          \"kilo\",\n",
    "          \"gram\",\n",
    "          \"sec\",\n",
    "          \"min\",\n",
    "          \"hour\",\n",
    "          \"hr\",\n",
    "          \"day\",\n",
    "          \"week\",\n",
    "          \"month\",\n",
    "          \"year\",\n",
    "          \"liter\",\n",
    "          \"litre\"]\n",
    "\n",
    "    abbr = [\"m\",\n",
    "          \"cm\",\n",
    "          \"mm\",\n",
    "          \"kg\",\n",
    "          \"g\",\n",
    "          \"mg\",\n",
    "          \"s\",\n",
    "          \"h\",\n",
    "          \"am\",\n",
    "          \"pm\",\n",
    "          \"l\",\n",
    "          \"ml\"]\n",
    "    \n",
    "    # --- insert white space in front of a unit where necessary\n",
    "    for u in unit:\n",
    "        txt = re.sub(\"(\\d)\" + u, \"\\\\1 \" + u, txt)\n",
    "\n",
    "    for a in abbr:\n",
    "        txt = re.sub(\"(\\d)\" + a, \"\\\\1 \" + a, txt)\n",
    "\n",
    "    # --- compress repetative punctuation into a single character\n",
    "    txt = re.sub(\"\\\\!+\", \"!\", txt)\n",
    "    txt = re.sub(\"\\\\?+\", \"?\", txt)\n",
    "    txt = re.sub(\"\\\\.+\", \".\", txt)\n",
    "    txt = re.sub(\"\\\\-+\", \"-\", txt)\n",
    "    txt = re.sub(\"_+\", \"_\", txt)\n",
    "    txt = re.sub(\"~+\", \"~\", txt)\n",
    "    txt = re.sub(\"kappaB\", \"kappa B\", txt)\n",
    "    txt = re.sub('([a-z0-9])/([a-z0-9])', '\\\\1 / \\\\2', txt, flags=re.IGNORECASE)\n",
    "    txt = re.sub(\"\\(\", \" ( \", txt, flags=re.IGNORECASE)\n",
    "    txt = re.sub(\"\\)\", \" ) \", txt, flags=re.IGNORECASE)\n",
    "    \n",
    "    # --- remove long gene sequences\n",
    "    txt = re.sub(\"[ACGT ]{6,}\", \"\", txt);\n",
    "\n",
    "    # --- normalise white spaces\n",
    "    txt = re.sub(\"\\\\s+\", \" \", txt)\n",
    "\n",
    "    # --- normalise non-ASCII characters\n",
    "    # ???: test with unicode characters\n",
    "    #txt = Normalizer.normalize(txt, Normalizer.Form.NFD);\n",
    "    #txt = txt.replaceAll(\"[^\\\\x00-\\\\x7F]\", \"\");\n",
    "   \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- remove a hyphen between 2 letters so that it does not mess up the tokenisation in spacy: -/HYPH\n",
    "# --- NOTE: not part of pretagging, because the hyphen is only ignored, not removed\n",
    "\n",
    "def hyphen(txt):\n",
    "    txt = re.sub('([a-z])\\\\-([a-z])', '\\\\1 \\\\2', txt, flags=re.IGNORECASE)\n",
    "    # --- repeat for overlapping matches, as only one gets replaced,\n",
    "    #     e.g. glutathione-S-transferase -> glutathione S-transferase -> glutathione S transferase\n",
    "    txt = re.sub('([a-z])\\\\-([a-z])', '\\\\1 \\\\2', txt, flags=re.IGNORECASE)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- generalise tags to simplify patterns (regex) specified in the settings\n",
    "\n",
    "def gtag(tag):\n",
    "\n",
    "    if (len(tag) <= 1):         tag = \"PUN\"\n",
    "    elif (tag == \"PRP$\"):       tag = \"PRP\"\n",
    "    elif (tag == \"WP$\"):        tag = \"WP\"\n",
    "    elif (tag.find(\"JJ\") == 0): tag = \"JJ\"\n",
    "    elif (tag.find(\"NN\") == 0): tag = \"NN\";\n",
    "    elif (tag.find(\"RB\") == 0): tag = \"RB\";\n",
    "    elif (tag.find(\"VB\") == 0): tag = \"VB\";\n",
    "    \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prepare lemma for stemming\n",
    "def prestem(lemma):\n",
    "    \n",
    "    if len(lemma) > 1: \n",
    "        if lemma[0:1] == '-': lemma = lemma[1:]    # --- strip of hyphen at the start\n",
    "\n",
    "    if len(lemma) > 1:\n",
    "        if lemma[-1:] == '-': lemma = lemma[:-1]   # --- strip of hyphen at the end\n",
    "    \n",
    "    lemma = re.sub('isation', 'ization', lemma)    # --- American spelling for consistent stemming\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load data\n",
    "\n",
    "#####\n",
    "cur1.execute(\"DELETE FROM data_document;\")\n",
    "cur1.execute(\"DELETE FROM data_sentence;\")\n",
    "cur1.execute(\"DELETE FROM data_token;\")\n",
    "#####\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# --- read documents from the \"text\" folder\n",
    "folder = \"./text\"\n",
    "print(\"Loading data from \" + folder + \"...\");\n",
    "n = 0\n",
    "for doc_id in os.listdir(folder):\n",
    "    n += 1\n",
    "    file_path = os.path.join(folder, doc_id)\n",
    "    if os.path.isfile(file_path):\n",
    "        print('.', end='')\n",
    "        file = open(file_path, \"r\", encoding=\"utf8\")\n",
    "        verbatim = file.read()\n",
    "        file.close()\n",
    "        content = pretagging(verbatim)\n",
    "        \n",
    "        row = (doc_id, content, verbatim)\n",
    "        cur1.execute(\"INSERT INTO data_document(id, document, verbatim) VALUES(?, ?, ?);\", row)\n",
    "        \n",
    "        # --- split sentences\n",
    "        s = 0\n",
    "        doc = nlp(hyphen(content))\n",
    "        for sent in doc.sents: # --- store sentences\n",
    "            s+=1\n",
    "            sentence = sent.text\n",
    "            tokens = \" \".join([token.text for token in sent])\n",
    "            for token in sent: \n",
    "                token.tag_ = gtag(token.tag_) # --- generalise tag, e.g. JJR --> JJ\n",
    "                # --- prevent tagging of symbols and abbreviations as NNs\n",
    "                if token.text == '%': token.tag_ = 'SYM'\n",
    "                elif token.text.lower() in ('et', 'al', 'etc'): token.tag_ = 'XX'\n",
    "                elif token.text.lower() in ('related', 'based'): token.tag_ = 'JJ'\n",
    "            tags = \" \".join([token.tag_ for token in sent])\n",
    "            tagged_sentence = \" \".join([token.text+\"/\"+token.tag_ for token in sent])\n",
    "            sentence_id = doc_id+\".\"+str(s)\n",
    "            row = (sentence_id, doc_id, s, sentence, tagged_sentence, tags)\n",
    "            cur1.execute(\"INSERT INTO data_sentence(id, doc_id, position, sentence, tagged_sentence, tags) VALUES(?, ?, ?, ?, ?, ?)\", row)\n",
    "            \n",
    "            # --- tokenise sentences\n",
    "            p = 0\n",
    "            for token in sent: # --- store tokens\n",
    "                p+=1\n",
    "                lemma = token.lemma_.lower()    # --- lemmatise\n",
    "                lemma = prestem(lemma)          # --- prepare lemma for stemming\n",
    "                stem = stemmer.stem(lemma)      # --- stem lemma\n",
    "                row = (sentence_id, p, token.text, stem, lemma, token.tag_)\n",
    "                cur1.execute(\"INSERT INTO data_token(sentence_id, position, token, stem, lemma, gtag) VALUES(?, ?, ?, ?, ?, ?)\", row)\n",
    "\n",
    "if n == 0:\n",
    "    con.close()\n",
    "    sys.exit('No input data found. Check the text folder.')\n",
    "                \n",
    "con.commit()\n",
    "    \n",
    "print('\\nData loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"CREATE INDEX idx01 ON data_document(id);\")\n",
    "cur1.execute(\"CREATE INDEX idx02 ON data_token(sentence_id, position);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Data loaded in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- extract term candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- extract NPs of a predefined structure (the pattern in the settings)\n",
    "\n",
    "#####\n",
    "cur1.execute(\"DELETE FROM term_phrase;\")\n",
    "#####\n",
    "\n",
    "print(\"Extracting term candidates...\");\n",
    "\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "cur1.execute(\"SELECT id, tags FROM data_sentence WHERE length(sentence) > 30;\") # --- extract POS tags\n",
    "rows1 = cur1.fetchall()\n",
    "total = len(rows1)\n",
    "n = 0\n",
    "for row1 in rows1:\n",
    "    \n",
    "    # --- progress bar\n",
    "    n += 1\n",
    "    sys.stdout.write('\\r')\n",
    "    p = int(100*n/total)\n",
    "    sys.stdout.write(\"[%-100s] %d%%\" % ('='*p, p))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    sentence_id = row1[0]\n",
    "    tags = row1[1]\n",
    "    # --- match patterns\n",
    "    for chunk in re.finditer(regex, tags):\n",
    "        start = tags[:chunk.span()[0]].count(' ')+1\n",
    "        length = tags[chunk.span()[0]:chunk.span()[1]].count(' ')+1\n",
    "        \n",
    "        # --- extract the corresponding tokens\n",
    "        cur2.execute(\"\"\"SELECT token\n",
    "                        FROM   data_token\n",
    "                        WHERE  sentence_id = ?\n",
    "                        AND    position >= ? \n",
    "                        AND    position < ?\n",
    "                        ORDER BY position ASC;\"\"\", (sentence_id, start, start+length))\n",
    "        rows2 = cur2.fetchall()\n",
    "        \n",
    "        # --- trim leading stopwords\n",
    "        tokens = []\n",
    "        for row2 in rows2: tokens.append(row2[0]) \n",
    "        i = 0\n",
    "        while length > 1:\n",
    "            if tokens[i].lower() in stopwords:\n",
    "                start += 1\n",
    "                length -= 1\n",
    "                i+=1\n",
    "            else: break\n",
    "        \n",
    "        tokens = tokens[i:]\n",
    "        \n",
    "        # --- trim trailing stopwords\n",
    "        i = len(tokens) - 1\n",
    "        while length > 1:\n",
    "            if tokens[i].lower() in stopwords:\n",
    "                length -= 1\n",
    "                i-=1\n",
    "            else: break\n",
    "\n",
    "        tokens = tokens[:i+1]\n",
    "        \n",
    "        # --- join tokens into a phrase\n",
    "        phrase = \" \".join(tokens)\n",
    "        phrase_id = sentence_id+\".\"+str(start)\n",
    "        \n",
    "        # --- if still multi-word phrase and not too long\n",
    "        if 1 < length and length < 8:\n",
    "            \n",
    "            # --- strip off possible . at the end\n",
    "            if phrase.endswith('.'): phrase = phrase[:-1]\n",
    "                \n",
    "            # --- ignore phrases that contain web concepts: email address, URL, #hashtag\n",
    "            if not(phrase.find(\"@\")>=0 or \n",
    "                   phrase.find(\"#\")>=0 or \n",
    "                   phrase.lower().find(\"http\")>=0 or \n",
    "                   phrase.lower().find(\"www\")>=0):\n",
    "                # --- normalise phrase by stemming\n",
    "                cur2.execute(\"\"\"SELECT DISTINCT stem\n",
    "                                FROM   data_token\n",
    "                                WHERE  sentence_id = ?\n",
    "                                AND    ? <= position AND position < ?\n",
    "                                EXCEPT SELECT word FROM stopword\n",
    "                                ORDER BY stem ASC;\"\"\", (sentence_id, start, start+length))\n",
    "                                ###AND    NOT (LOWER(token) = token AND LENGTH(token) < 3)\n",
    "                rows2 = cur2.fetchall()\n",
    "                stems = []\n",
    "                for row2 in rows2: stems.append(row2[0])\n",
    "                normalised = \" \".join(stems)\n",
    "                normalised = normalised.replace('.', '') # --- e.g. U.K., Dr., St. -> UK, Dr, St\n",
    "                \n",
    "                # --- store phrase as a MWT candidate\n",
    "                cur2.execute(\"\"\"INSERT INTO term_phrase(id, sentence_id, token_start, token_length, phrase, normalised)\n",
    "                                VALUES (?,?,?,?,?,?);\"\"\", (phrase_id, sentence_id, start, length, phrase, normalised))\n",
    "\n",
    "cur1.execute(\"UPDATE term_phrase SET flat = LOWER(REPLACE(phrase, ' ', ''));\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur1.execute(\"CREATE INDEX idx03 ON term_phrase(flat);\")\n",
    "cur1.execute(\"CREATE INDEX idx04 ON term_phrase(LOWER(phrase));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Term candidates extracted in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- re-normalise term candidates that have different TOKENISATION,\n",
    "#     e.g. posterolateral corner B vs. postero lateral corner\n",
    "# --- keep the one with MORE tokens (e.g. postero lateral corner)\n",
    "\n",
    "cur1.execute(\"DELETE FROM tmp_normalised;\")\n",
    "\n",
    "cur1.execute(\"\"\"INSERT INTO tmp_normalised(changeto, changefrom)\n",
    "                SELECT P1.normalised, P2.normalised\n",
    "                FROM   term_phrase P1, term_phrase P2\n",
    "                WHERE  P1.flat = P2.flat\n",
    "                AND    P1.token_length > P2.token_length\n",
    "                AND    P1.normalised <> P2.normalised;\"\"\")\n",
    "\n",
    "cur1.execute(\"\"\"SELECT DISTINCT changefrom, changeto FROM tmp_normalised;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    changefrom = row1[0]\n",
    "    changeto   = row1[1]\n",
    "    print(changefrom, \"-->\", changeto)\n",
    "    cur2.execute(\"UPDATE term_phrase SET normalised = ? WHERE normalised = ?;\", (changeto, changefrom))\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Term candidates normalised in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- acronym recognition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- assumption: acronyms are explicitly defined in text, e.g. \n",
    "#     ... blah blah retinoic acid receptor (RAR) blah blah ...\n",
    "#                   ~~~~~~~~~~~~~~~~~~~~~~  ~~~\n",
    "\n",
    "# --- based on this paper:\n",
    "#     Schwartz A & Hearst M (2003) \n",
    "#     A simple algorithm for identifying abbreviation definitions in biomedical text, \n",
    "#     Pacific Symposium on Biocomputing 8:451-462 [http://biotext.berkeley.edu/software.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- alpha -> a: helps properly estimate the acronym length and simplifies matching against the long form\n",
    "\n",
    "def pad(string):\n",
    "    return \" \" + string + \" \"\n",
    "\n",
    "def greek2english(string):\n",
    "    \n",
    "    letters = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zata\", \"eta\", \"theta\", \"iota\",\"kappa\", \"lambda\", \"mu\", \"nu\", \"xi\", \"omikron\", \"pi\", \"rho\", \"sigma\", \"tau\", \"upsilon\", \"phi\",\"chi\", \"psi\", \"omega\"]\n",
    "    \n",
    "    string = pad(string)\n",
    "    \n",
    "    for letter in letters:\n",
    "        string = re.sub(pad(letter), pad(letter[0:1]), string, flags=re.IGNORECASE)\n",
    "\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- checks if a string looks like an acronym\n",
    "\n",
    "def isValidShortForm(string):\n",
    "\n",
    "    string = greek2english(string)\n",
    "    \n",
    "    if len(string) < 2:                                                                       # --- acronym too short\n",
    "        return False\n",
    "    elif len(string) > 8:                                                                     # --- acronym too long\n",
    "        return False\n",
    "    elif not(any(char.isupper() for char in string)):                                         # --- no uppercase\n",
    "        return False\n",
    "    elif (sum([int(c.islower()) for c in string]) > sum([int(c.isupper()) for c in string])): # --- more lowercase than uppercase\n",
    "        return False\n",
    "    elif not(string[0].isalpha() or string[0].isdigit() or string[0] == '('):                 # --- invalid first character\n",
    "        return False\n",
    "    elif (len(re.sub('[a-z0-9\\s\\'/\\-]', '', string.lower())) > 0):                            # --- invalid characters present\n",
    "        return False\n",
    "    elif string[1] == \"'\":                                                                    # --- 2nd character ' as in A'\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- processes the context (i.e. definition) to extract the best long form for a given acronym\n",
    "\n",
    "def bestLongForm(acronym, definition):\n",
    "    \n",
    "    # --- case-insensitive matching\n",
    "    acronym = acronym.replace(\"-\", \"\").lower()\n",
    "    definition = definition.lower()\n",
    "    d = len(definition) - 1\n",
    "\n",
    "    # --- go through the acronym & definition character by character,\n",
    "    #     FROM RIGHT TO LEFT looking for a match\n",
    "\n",
    "    for a in range(len(acronym) - 1, -1, -1):\n",
    "        \n",
    "        c = acronym[a]\n",
    "\n",
    "        if (c.isalpha() or c.isdigit()):    # --- match an alphanumeric character\n",
    "            \n",
    "            while ((d >= 0 and definition[d] != c) or (a == 0 and d > 0 and (definition[d-1].isalpha() or definition[d-1].isdigit()))):\n",
    "                d -= 1                      # --- keep moving to the left\n",
    "\n",
    "        if (d < 0):                         # --- match failed\n",
    "            return None\n",
    "        else:                               # --- match found\n",
    "            d -= 1                          # --- skip the matching character and then continue matching\n",
    "\n",
    "    d = definition.rfind(' ', 0, d+1) + 1   # --- complete the left-most word (up to the white space)\n",
    "\n",
    "    definition = definition[d:].strip()     # --- delete the surplus text on the left\n",
    "\n",
    "    if definition.startswith('an '):     definition = definition[3:]    # --- starts with a determiner?\n",
    "    elif definition.startswith('a '):    definition = definition[2:]\n",
    "    elif definition.startswith('the '):  definition = definition[4:]\n",
    "    elif (definition.startswith('[') and definition.endswith(']')):     # --- [definition]\n",
    "        definition = definition[1:-1]\n",
    "    elif (definition.startswith(\"'\") and definition.endswith(\"'\")):     # --- 'definition'\n",
    "        definition = definition[1:-1]\n",
    "\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- extracts all potential (acronym, definition) pairs from a given sentence\n",
    "\n",
    "def extractPairs(sentence):\n",
    "    \n",
    "    pairs = []\n",
    "\n",
    "    # --- remove double quotes\n",
    "    sentence = sentence.replace('\"', ' ')\n",
    "    \n",
    "    # --- normalise white spaces\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    \n",
    "    acronym = ''\n",
    "    definition = ''\n",
    "    o = sentence.find(' (')   # --- find (\n",
    "    c = -1                    # --- ) index\n",
    "    tmp = -1\n",
    "\n",
    "    while (1 == 1):\n",
    "\n",
    "        if (o > -1):\n",
    "            o +=1                       # --- skip white space, i.e. ' (' -> '('\n",
    "            c = sentence.find(')', o)   # --- find closed parenthesis\n",
    "            \n",
    "            # --- extract candidates for (acronym, definition)\n",
    "            if (c > -1):\n",
    "                # --- find the start of the previous clause based on punctuation\n",
    "                cutoff = max(sentence.rfind('. ', 0, o), sentence.rfind(', ', 0, o))\n",
    "                if (cutoff == -1): cutoff = -2\n",
    "\n",
    "                definition = sentence[cutoff + 2:o].strip()\n",
    "                acronym = sentence[o + 1:c].strip()\n",
    "        \n",
    "        if (len(acronym) > 0 or len(definition) > 0): # --- candidates successfully instantiated above\n",
    "\n",
    "            if (len(acronym) > 1 and len(definition) > 1):\n",
    "                # --- look for parentheses nested within the candidate acronym\n",
    "                nextc = sentence.find(')', c + 1)\n",
    "                if (acronym.find('(') > -1 and nextc > -1):\n",
    "                    acronym = sentence[o + 1:nextc]\n",
    "                    c = nextc\n",
    "\n",
    "                # --- if separator found within parentheses, then trim everything after it\n",
    "                tmp = acronym.find(', ')\n",
    "                if (tmp > -1): acronym = acronym[0:tmp]\n",
    "                tmp = acronym.find('; ')\n",
    "                if (tmp > -1): acronym = acronym[0:tmp]\n",
    "                tmp = acronym.find(' or ')\n",
    "                if (tmp > -1): acronym = acronym[0:tmp]\n",
    "                if (tmp > -1): acronym = acronym[0:tmp]\n",
    "\n",
    "                # --- (or ...) -> (...)\n",
    "                tmp = acronym.find('or ')\n",
    "                if (tmp == 0): acronym = acronym[3:]\n",
    "\n",
    "                tokens = acronym.split()\n",
    "                if (len(tokens) > 3 or len(acronym) > len(definition)):\n",
    "                    # --- definition found within (...)\n",
    "                \n",
    "                    # --- extract the last token before \"(\" as a candidate for acronym\n",
    "                    tmp = sentence.rfind(' ', 0, o - 2)\n",
    "                    substr = sentence[tmp + 1:o - 1]\n",
    "                \n",
    "                    # --- swap acronym & definition\n",
    "                    definition = acronym\n",
    "                    acronym = substr\n",
    "                    \n",
    "                    # --- validate (... definition ...)\n",
    "                    if (len(definition.replace('-', ' ').split(' ')) > len(acronym) + 2):\n",
    "                        acronym = '' # --- delete acronym\n",
    "\n",
    "            acronym = acronym.strip()\n",
    "            definition = definition.strip()\n",
    "\n",
    "            if (isValidShortForm(acronym)):\n",
    "                blf = matchPair(acronym, definition)\n",
    "                if blf != None: \n",
    "\n",
    "                    # --- NOTE: blf is already in lowercase\n",
    "                    \n",
    "                    pairs.append([acronym, blf])\n",
    "\n",
    "            # --- prepare to process the rest of the sentence after \")\"\n",
    "            sentence = sentence[c + 1:]\n",
    "\n",
    "        elif (o > -1): sentence = sentence[o + 1:] # --- process the rest of the sentence\n",
    "\n",
    "        acronym = ''\n",
    "        definition = ''\n",
    "    \n",
    "        o = sentence.find(' (')\n",
    "        if o < 0: return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- finds the best match for an acronym and checks if it looks like a valid long form\n",
    "\n",
    "def matchPair(acronym, definition):\n",
    "    \n",
    "    # --- abort if acronym too short\n",
    "    if (len(acronym) < 2): return None\n",
    "    \n",
    "    # --- find the long form\n",
    "    blf = bestLongForm(acronym, definition)\n",
    "\n",
    "    # --- abort if no long form found\n",
    "    if (blf == None): return None\n",
    "\n",
    "    # --- t = the number of tokens in the long form\n",
    "    t = len(blf.replace('-', ' ').split(' '))\n",
    "    \n",
    "    # --- c = the number of alphanumeric characters in the acronym\n",
    "    c = sum([int(char.isalpha() or char.isdigit()) for char in acronym])\n",
    "\n",
    "    # --- case-insensitive matching; NOTE: blf is already in lowercase\n",
    "    acronym = acronym.lower().replace(' ', '')\n",
    "    \n",
    "    # --- sanity check\n",
    "    if len(blf) < 8:                           # --- long form too short\n",
    "        return None\n",
    "    elif len(blf) <= len(acronym):             # --- long form < short form\n",
    "        return None\n",
    "    elif blf.startswith(acronym + ' '):        # --- acronym nested in the long form\n",
    "        return None\n",
    "    elif blf.find(' ' + acronym + ' ') > -1:   # --- acronym nested in the long form\n",
    "        return None\n",
    "    elif blf.endswith(' ' + acronym):          # --- acronym nested in the long form\n",
    "        return None\n",
    "    elif acronym[0:1] != blf[0:1]:             # --- they don't start with the same letter\n",
    "        return None\n",
    "    elif t > 2*c or t > c+5:                   # --- too many tokens in the long form\n",
    "        return None\n",
    "    elif blf.find('[') >= 0 or blf.find(']') >= 0:\n",
    "        return None\n",
    "    else:                                       # --- no match in the last two tokens\n",
    "        tokens = blf.split()\n",
    "        if len(tokens) > 2:\n",
    "            last2 = \" \".join(tokens[-2:])\n",
    "            if last2 == last2.replace(acronym[-1], \"\"):\n",
    "                return None\n",
    "\n",
    "        # --- delete all other letters from the definition: a token with no match will disappear\n",
    "        remainder = re.sub(\"[^ \"+acronym.replace('-', '')+\"]\", \"\", blf)\n",
    "        tokens = len(remainder.split())\n",
    "        #if len(acronym) - tokens >= 2:          # --- at least two unmatched tokens\n",
    "        if len(blf.split()) - tokens >= 2:       # --- at least two unmatched tokens\n",
    "            return None\n",
    "           \n",
    "    return blf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- explicit acronym recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compare two acronym definitions and return the preferred one\n",
    "\n",
    "def preferred(acronym, definition1, definition2):\n",
    "    # --- lemmatise and lowercase both definitions\n",
    "    def1 = \" \".join([token.lemma_.lower() for token in nlp(definition1.replace('-', ' '))])\n",
    "    def2 = \" \".join([token.lemma_.lower() for token in nlp(definition2.replace('-', ' '))])\n",
    "    \n",
    "    def1_def2 = pad(def1)\n",
    "    for token in def2.split(): def1_def2 = re.sub(pad(token), \" \", def1_def2)\n",
    "    def1_def2 = def1_def2.strip()\n",
    "    \n",
    "    def2_def1 = pad(def2)\n",
    "    for token in def1.split(): def2_def1 = re.sub(pad(token), \" \", def2_def1)\n",
    "    def2_def1 = def2_def1.strip()\n",
    "\n",
    "    if def1_def2 == \"\":                                     # --- nuclear factor kappa B vs nuclear REGULATORY factor kappa B\n",
    "        if len(acronym) == len(def2.split()):               # --- prefer potential initialism\n",
    "            return definition2\n",
    "        else: \n",
    "            return definition1\n",
    "    elif def2_def1 == \"\":                                   # --- nuclear REGULATORY factor kappa B vs nuclear factor kappa B\n",
    "        if len(acronym) == len(def1.split()):               # --- prefer potential initialism\n",
    "            return definition1\n",
    "        else: \n",
    "            return definition2\n",
    "    elif bestLongForm(def1_def2, def2_def1) == def2_def1:   # --- GC receptor vs glucocorticoid receptor\n",
    "        return definition2\n",
    "    elif bestLongForm(def2_def1, def1_def2) == def1_def2:   # --- glucocorticoid receptor vs GC receptor\n",
    "        return definition1\n",
    "    else:\n",
    "        sim = jellyfish.jaro_winkler_similarity(def1, def2)\n",
    "        if sim < 0.7:                                       # --- ambiguous acronym\n",
    "            return 'xxx'\n",
    "        elif len(def1_def2) < len(def2_def1):               # --- keep the shorter one\n",
    "            return definition1\n",
    "        else:\n",
    "            return definition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_acronyms():\n",
    "    ###\n",
    "    cur1.execute(\"DELETE FROM term_acronym;\")\n",
    "    ###\n",
    "\n",
    "    dictionary = {} # --- create a JSON dictionary of short/long forms\n",
    "\n",
    "    # --- extract sentences that contain a pair of parentheses, e.g.\n",
    "    #     ... blah blah ( blah blah ) blah blah ...\n",
    "\n",
    "    cur1.execute(\"SELECT sentence FROM data_sentence WHERE tags LIKE '%-LRB- % -RRB-%';\")\n",
    "    rows1 = cur1.fetchall()\n",
    "    for row1 in rows1:\n",
    "        sentence = row1[0]\n",
    "\n",
    "        # --- extract all acronym definitions\n",
    "        pairs = extractPairs(sentence)\n",
    "    \n",
    "        for i in range(len(pairs)):\n",
    "            # --- parse definition by spacy so that it is comparable to previously extracted MWT candidates\n",
    "            definition = nlp(pairs[i][1])\n",
    "            # --- store definition to the dictionary\n",
    "            acronym = pairs[i][0]\n",
    "            value = \" \".join([token.text for token in definition])\n",
    "            cur2.execute(\"INSERT INTO tmp_acronym(acronym, phrase) VALUES(?,?);\", (acronym, value)) # --- for debugging\n",
    "            if acronym in dictionary.keys():\n",
    "                dictionary[acronym] = preferred(acronym, value, dictionary[acronym])\n",
    "            else:\n",
    "                dictionary[acronym] = value\n",
    "\n",
    "    # --- print dictionary to log\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(dictionary)\n",
    "\n",
    "    # --- store acronyms as MWT candidates\n",
    "    for key in dictionary.keys():\n",
    "        phrase = dictionary[key]\n",
    "        if phrase != 'xxx': # --- ignore ambiguous acronyms\n",
    "            cur1.execute(\"\"\"INSERT INTO term_acronym(acronym, phrase, normalised)\n",
    "                            SELECT DISTINCT ?, ?, normalised\n",
    "                            FROM   term_phrase\n",
    "                            WHERE  LOWER(?) = LOWER(phrase);\"\"\", (key, phrase, phrase))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- implicit acronym recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- assumptions: \n",
    "#     (1) acronyms are frequently used\n",
    "#     (2) expanded form also used in the corpus, but\n",
    "#        these two are probably not linked explicitly\n",
    "#        e.g. blah ACL blah blah ACL blah blah anterior cruciate ligament blah blah \n",
    "#                  ~~~           ~~~           ~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# --- find tokens that are potential acronyms:\n",
    "#     (1) must contain an UPPERCASE letter, but no lowercase letters\n",
    "#     (2) must not start with - (avoids e.g. -LRB-)\n",
    "#     (3) must not end with . (avoids MR. so and so)\n",
    "#     (4) has to be at least 3 characters long as shorter ones are \n",
    "#         likely to introduce false positive expanded forms as they \n",
    "#         are more likely to match a random phrase as an expanded form \n",
    "#         candidate\n",
    "#     (5) acronyms are frequently used, so a threshold is set to >MIN times\n",
    "\n",
    "def implicit_acronyms():\n",
    "    cur1.execute(\"DELETE FROM tmp_acronym;\")\n",
    "    cur1.execute(\"DELETE FROM term_acronym;\")\n",
    "\n",
    "    # --- find tokens that look like acronyms\n",
    "    cur1.execute(\"\"\"SELECT token, COUNT(*)\n",
    "                    FROM   data_token\n",
    "                    WHERE  UPPER(token) = token\n",
    "                    AND    LENGTH(token) < 6\n",
    "                    AND    token GLOB '[A-Z][A-Z]*[A-Z]'\n",
    "                    GROUP BY token\n",
    "                    HAVING COUNT(*) > ?;\"\"\", (Amin,))\n",
    "    rows1 = cur1.fetchall()\n",
    "    for row1 in rows1:\n",
    "        acronym = row1[0]\n",
    "        length  = len(acronym)\n",
    "        pattern = \"\"\n",
    "    \n",
    "        # --- create a LIKE pattern to retrieve matching phrases\n",
    "        for i in range (0, length): pattern += acronym[i] + \"% \"\n",
    "        pattern = pattern.strip()\n",
    "\n",
    "        # --- extract potential expanded forms\n",
    "        cur2.execute(\"\"\"INSERT INTO tmp_acronym(acronym, normalised)\n",
    "                        SELECT DISTINCT ?, normalised\n",
    "                        FROM   term_phrase\n",
    "                        WHERE  phrase LIKE ?\n",
    "                        AND    LENGTH(phrase) - LENGTH(REPLACE(phrase, ' ', '')) = ? - 1;\"\"\", (acronym, pattern, length))\n",
    "\n",
    "    # --- check number of senses\n",
    "    cur1.execute(\"SELECT acronym, COUNT(*) FROM tmp_acronym GROUP BY acronym;\")\n",
    "    rows1 = cur1.fetchall()\n",
    "    for row1 in rows1:\n",
    "        acronym = row1[0]\n",
    "        senses = row1[1]\n",
    "        \n",
    "        cosine = {} # --- calculate cosine similarity based on verbs that co-occur in the same sentence\n",
    "\n",
    "        # --- sqrt(sum(verb:count^2)) for the acronym\n",
    "        cur2.execute(\"DELETE FROM v1;\")\n",
    "        cur2.execute(\"\"\"INSERT INTO v1(lemma, value)\n",
    "                        SELECT lemma, COUNT(*)\n",
    "                        FROM   data_token\n",
    "                        WHERE  sentence_id IN (SELECT sentence_id FROM data_token WHERE token=?)\n",
    "                        AND    gtag = 'VB' AND lemma NOT IN ('be', 'have', 'do') GROUP BY lemma;\"\"\", (acronym,))\n",
    "        cur2.execute(\"SELECT SUM(value*value) FROM v1;\")\n",
    "        norm1 = cur2.fetchone()[0]\n",
    "        if norm1 != None: norm1 = math.sqrt(norm1)\n",
    "        else:             norm1 = 0\n",
    "        \n",
    "        if norm1 > 0:\n",
    "            # --- for each sense\n",
    "            cur2.execute(\"SELECT normalised FROM tmp_acronym WHERE acronym = ?;\", (acronym,))\n",
    "            rows2 = cur2.fetchall()\n",
    "            for row2 in rows2:\n",
    "                \n",
    "                # --- get sense\n",
    "                normalised = row2[0]\n",
    "                \n",
    "                # --- frequency of occurrence\n",
    "                cur3.execute(\"SELECT COUNT(*) FROM term_phrase WHERE normalised = ?;\", (normalised,))\n",
    "                f = cur3.fetchone()[0]\n",
    "            \n",
    "                if f > 1: # --- ignore single occurrences (outliers)\n",
    "                    \n",
    "                    # --- sqrt(sum(verb:count^2)) for the sense\n",
    "                    cur3.execute(\"DELETE FROM v2;\")\n",
    "                    cur3.execute(\"\"\"INSERT INTO v2(lemma, value)\n",
    "                                    SELECT lemma, COUNT(*)\n",
    "                                    FROM   data_token\n",
    "                                    WHERE  sentence_id IN (SELECT sentence_id FROM term_phrase WHERE normalised = ?)\n",
    "                                    AND    gtag = 'VB' AND lemma NOT IN ('be', 'have', 'do') GROUP BY lemma;\"\"\", (normalised,))\n",
    "                    cur3.execute(\"SELECT SUM(value*value) FROM v2;\")\n",
    "                    norm2 = cur3.fetchone()[0]\n",
    "                    if norm2 != None: norm2 = math.sqrt(norm2)\n",
    "                    else:             norm2 = 0\n",
    "                    \n",
    "                    if norm2 > 0:\n",
    "                        # --- scalar product: sum(verb:count_acronym*verb:count_sense)\n",
    "                        cur3.execute(\"\"\"SELECT SUM(v1.value * v2.value)\n",
    "                                        FROM   v1, v2\n",
    "                                        WHERE  v1.lemma = v2.lemma;\"\"\")\n",
    "                        product = cur3.fetchone()[0]\n",
    "                        if product != None: cosine[normalised] = product / (norm1*norm2)\n",
    "                        else:               cosine[normalised] = 0\n",
    "\n",
    "        if cosine:\n",
    "            # --- find the most similar sense\n",
    "            normalised = max(cosine, key=lambda k: cosine[k])\n",
    "            similarity = max(cosine.values())\n",
    "        \n",
    "            # --- find the most common phrase for the given normalised form\n",
    "            cur2.execute(\"\"\"SELECT LOWER(phrase), COUNT(*) AS C\n",
    "                            FROM   term_phrase\n",
    "                            WHERE  normalised = ?\n",
    "                            ORDER BY C DESC;\"\"\", (normalised,))\n",
    "            phrase = cur2.fetchone()[0]\n",
    "\n",
    "            print(acronym, '\\t', phrase, '\\t', normalised, '\\t', similarity)\n",
    "        \n",
    "            # --- store acronym definition\n",
    "            cur2.execute(\"\"\"INSERT INTO term_acronym(acronym, phrase, normalised) VALUES(?,?,?);\"\"\", (acronym, phrase, normalised))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- extract acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acronyms == \"explicit\": \n",
    "    print(\"Extracting explicit acronyms...\")\n",
    "    explicit_acronyms()\n",
    "else:\n",
    "    print(\"Extracting implicit acronyms...\")\n",
    "    implicit_acronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Acronyms extracted in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- integrate acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- expand definitions that contain other acronyms, e.g. \n",
    "#     NIK = NF kappa B inducing kinase -> nuclear factor kappa B inducing kinase\n",
    "cur1.execute(\"DELETE FROM tmp_normalised;\")\n",
    "\n",
    "cur1.execute(\"\"\"SELECT DISTINCT LOWER(A.acronym), A.normalised, LOWER(P.phrase), P.normalised\n",
    "                FROM   term_acronym A, term_acronym P\n",
    "                WHERE  ' ' || P.phrase || ' ' LIKE '% ' || A.acronym || ' %';\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    acronym = row1[0].split()\n",
    "    definition = row1[1].split()\n",
    "    phrase = row1[2]\n",
    "    normalised = row1[3].split()\n",
    "    normalised = np.setdiff1d(normalised, acronym)\n",
    "    normalised = np.union1d(normalised, definition)\n",
    "    renormalised = \" \".join(np.sort(normalised))\n",
    "\n",
    "    cur2.execute(\"INSERT INTO tmp_normalised(changefrom, changeto) VALUES(?, ?);\", (phrase, renormalised))\n",
    "\n",
    "cur1.execute(\"SELECT changefrom, changeto FROM tmp_normalised;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    phrase = row1[0]\n",
    "    normalised = row1[1]\n",
    "    cur2.execute(\"UPDATE term_acronym SET normalised = ? WHERE LOWER(phrase) = ?;\", (normalised, phrase))\n",
    "\n",
    "    \n",
    "# --- treat acronyms that are NOT already NESTED within multi-word term candidates\n",
    "#     as stand-alone MWT candidates\n",
    "# --- insert mentions of such acronyms into the term_phrase table    \n",
    "\n",
    "cur1.execute(\"\"\"INSERT INTO term_phrase(id, sentence_id, token_start, token_length, phrase, normalised)\n",
    "                SELECT sentence_id || '.' || position, sentence_id, position, 1, acronym, normalised\n",
    "                FROM   data_token T, term_acronym A\n",
    "                WHERE  T.token = A.acronym\n",
    "                AND    T.gtag != 'IN'\n",
    "                EXCEPT\n",
    "                SELECT T.sentence_id || '.' || T.position, T.sentence_id, T.position, 1, A.acronym, A.normalised\n",
    "                FROM   data_token T, term_acronym A, term_phrase P\n",
    "                WHERE  T.token = A.acronym\n",
    "                AND    T.sentence_id = P.sentence_id\n",
    "                AND    P.token_start <= T.position\n",
    "                AND    T.position < P.token_start + P.token_length;\"\"\")\n",
    "\n",
    "\n",
    "        \n",
    "# --- now replace NESTED mentions of acronyms with their EXPANDED FORMS\n",
    "cur1.execute(\"DELETE FROM tmp_normalised;\")\n",
    "\n",
    "cur1.execute(\"\"\"SELECT DISTINCT LOWER(P.phrase), P.normalised\n",
    "                FROM   term_acronym A, term_phrase P\n",
    "                WHERE  P.normalised <> A.normalised\n",
    "                AND    ' ' || P.phrase || ' ' LIKE '% ' || LOWER(A.acronym) || ' %';\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    phrase = row1[0]\n",
    "    normalised = row1[1].split()\n",
    "    cur2.execute(\"\"\"SELECT LOWER(acronym) AS acr, normalised\n",
    "                    FROM   term_acronym\n",
    "                    WHERE  ' ' || ? || ' ' LIKE '% ' || acr || ' %'\n",
    "                    ORDER BY LENGTH(acronym) DESC;\"\"\", (phrase,))\n",
    "    rows2 = cur2.fetchall()\n",
    "    for row2 in rows2:\n",
    "        acronym = row2[0].split()\n",
    "        definition = row2[1].split()\n",
    "        normalised = np.setdiff1d(normalised, acronym)\n",
    "        normalised = np.union1d(normalised, definition)\n",
    "\n",
    "    renormalised = \" \".join(np.sort(normalised))\n",
    "\n",
    "    cur2.execute(\"INSERT INTO tmp_normalised(changefrom, changeto) VALUES(?, ?);\", (phrase, renormalised))\n",
    "\n",
    "cur1.execute(\"SELECT changefrom, changeto FROM tmp_normalised;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    phrase = row1[0]\n",
    "    normalised = row1[1]\n",
    "    cur2.execute(\"UPDATE term_phrase SET normalised = ? WHERE LOWER(phrase) = ?;\", (normalised, phrase))\n",
    "\n",
    "    \n",
    "# --- update multi-word acronyms, which were previously picked up as MWT candidates\n",
    "cur1.execute(\"SELECT LOWER(acronym), normalised FROM term_acronym WHERE acronym LIKE '% %';\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    acronym = row1[0]\n",
    "    token_length = len(acronym.split())\n",
    "    normalised = row1[1]\n",
    "    cur2.execute(\"UPDATE term_phrase SET normalised = ? WHERE LOWER(phrase) = ?;\", (normalised, acronym))\n",
    "    \n",
    "# --- add previously missed MWT candidates\n",
    "    extra = 0\n",
    "    cur2.execute(\"\"\"SELECT COUNT(*)\n",
    "                    FROM   data_sentence \n",
    "                    WHERE  ' ' || sentence || ' ' LIKE '% '|| ? ||' %';\"\"\", (acronym,))\n",
    "    extra += cur2.fetchone()[0]\n",
    "    \n",
    "    cur2.execute(\"\"\"SELECT COUNT(*)\n",
    "                    FROM   term_phrase\n",
    "                    WHERE  ' ' || phrase || ' ' LIKE '% '|| ? ||' %';\"\"\", (acronym,))\n",
    "    extra -= cur2.fetchone()[0]\n",
    "\n",
    "    while extra > 0:\n",
    "        cur2.execute(\"\"\"INSERT INTO term_phrase(id, sentence_id, token_start, token_length, phrase, normalised)\n",
    "                        VALUES(?,0,0,?,?,?);\"\"\", (acronym+'.'+str(extra),token_length,acronym,normalised))\n",
    "        extra -= 1\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Acronyms integrated in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- normalise MWT candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"DELETE FROM tmp_normalised;\")\n",
    "\n",
    "cur1.execute(\"\"\"INSERT INTO tmp_normalised(changefrom, changeto)\n",
    "                SELECT DISTINCT P1.normalised, P2.normalised\n",
    "                FROM   term_phrase P1, term_phrase P2\n",
    "                WHERE  P1.flat LIKE '%-%'\n",
    "                AND    REPLACE(LOWER(P1.phrase),'-',' ') = LOWER(P2.phrase);\"\"\")\n",
    "\n",
    "cur1.execute(\"\"\"SELECT DISTINCT changefrom, changeto FROM tmp_normalised;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    changefrom = row1[0]\n",
    "    changeto   = row1[1]\n",
    "    print(changefrom, \"-->\", changeto)\n",
    "    cur2.execute(\"UPDATE term_phrase SET normalised = ? WHERE normalised = ?;\", (changeto, changefrom))\n",
    "\n",
    "cur1.execute(\"DELETE FROM tmp_normalised;\")\n",
    "\n",
    "cur1.execute(\"\"\"INSERT INTO tmp_normalised(changeto, changefrom)\n",
    "                SELECT DISTINCT P1.normalised, P2.normalised\n",
    "                FROM   term_phrase P1, term_phrase P2\n",
    "                WHERE  P1.flat LIKE '%-%'\n",
    "                AND    REPLACE(LOWER(P1.phrase),'-','') = LOWER(P2.phrase)\n",
    "                AND    REPLACE(P1.normalised,'-','') = P2.normalised;\"\"\")\n",
    "\n",
    "cur1.execute(\"\"\"SELECT DISTINCT changefrom, changeto FROM tmp_normalised;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    changefrom = row1[0]\n",
    "    changeto   = row1[1]\n",
    "    print(changefrom, \"-->\", changeto)\n",
    "    cur2.execute(\"UPDATE term_phrase SET normalised = ? WHERE normalised = ?;\", (changeto, changefrom))\n",
    "    \n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"DELETE FROM term_normalised;\")\n",
    "cur1.execute(\"DELETE FROM term_bag;\")\n",
    "cur1.execute(\"DELETE FROM token;\")\n",
    "cur1.execute(\"DELETE FROM token_similarity;\")\n",
    "\n",
    "# --- select normalised MWT candidates\n",
    "cur1.execute(\"\"\"INSERT INTO term_normalised(normalised)\n",
    "                SELECT normalised FROM (\n",
    "                SELECT normalised, COUNT(*) AS t\n",
    "                FROM   term_phrase\n",
    "                WHERE  LENGTH(normalised) > 5\n",
    "                AND    normalised GLOB '[a-z0-9]*'\n",
    "                AND    normalised LIKE '% %'\n",
    "                GROUP BY normalised\n",
    "                HAVING t > 1);\"\"\")\n",
    "\n",
    "# --- tokenise normalised MWT candidates\n",
    "cur1.execute(\"SELECT rowid, normalised FROM term_normalised;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    id = row1[0]\n",
    "    normalised = row1[1]\n",
    "    \n",
    "    # --- tokenise normalised form\n",
    "    tokens = normalised.split()\n",
    "    \n",
    "    # --- store tokens as a bag of words\n",
    "    for token in tokens:\n",
    "        cur2.execute(\"INSERT INTO term_bag(id, token) VALUES(?,?);\", (id, token))\n",
    "\n",
    "    cur2.execute(\"UPDATE term_normalised SET len = ? WHERE rowid = ?;\", (len(tokens), id))\n",
    "        \n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Term candidates re-normalised in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- normalise tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- extract vocabulary of MWT candidates, i.e. select distinct tokens\n",
    "cur1.execute(\"INSERT INTO token(token) SELECT DISTINCT token FROM term_bag;\")\n",
    "\n",
    "# --- index tokens for faster retrieval\n",
    "cur1.execute(\"CREATE INDEX idx05 ON token(token);\")\n",
    "\n",
    "# --- compare tokens so that similar ones can be normalised\n",
    "# --- NOTE: for efficiency, only tokens of similar length that start with \n",
    "#           the same letter or potential ligature (ae, oe) are compared\n",
    "cur1.execute(\"\"\"SELECT T1.token AS t1, T2.token AS t2\n",
    "                FROM   token T1, token T2\n",
    "                WHERE  t1 < t2\n",
    "                AND    (SUBSTR(t1,1,1) = SUBSTR(t2,1,1) OR (SUBSTR(t1,1,1) = 'e' AND SUBSTR(t2,1,1) IN ('a', 'o')))\n",
    "                AND    ABS(LENGTH(t1) - LENGTH(t2)) < 2;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    t1 = row1[0]\n",
    "    t2 = row1[1]\n",
    "    if not any(c.isdigit() for c in t1+t2): # --- ignore tokens that contain digits as these may be significant\n",
    "        \n",
    "        # --- calculate token similarity\n",
    "        sim = jellyfish.jaro_winkler_similarity(t1, t2)\n",
    "        if sim > Smin: # --- token similarity threshold\n",
    "            cur2.execute(\"INSERT INTO token_similarity(token1, token2) VALUES(?,?)\",(t1,t2))\n",
    "\n",
    "# --- A -> B, B -> C, A -> C, then ignore B -> C and use A to normalise both B and C\n",
    "cur1.execute(\"\"\"SELECT token1 AS t1, token2 AS t2\n",
    "                FROM   token_similarity\n",
    "                EXCEPT\n",
    "                SELECT S2.token1 AS t1, S2.token2 AS t2\n",
    "                FROM   token_similarity S1, token_similarity S2\n",
    "                WHERE  S1.token2 = S2.token1;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    changeto   = row1[0]\n",
    "    changefrom = row1[1]\n",
    "    print(changefrom, \"\\t-->\", changeto)\n",
    "    cur2.execute(\"UPDATE term_bag SET token = ? WHERE token = ?\", (changeto, changefrom))\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- speed up searching through the bags of words\n",
    "cur1.execute(\"CREATE INDEX idx06 ON term_bag(id);\")\n",
    "cur1.execute(\"CREATE INDEX idx07 ON term_bag(id, token);\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- re-normalise the MWT candidates using similar tokens\n",
    "total = 0\n",
    "cur1.execute(\"SELECT MAX(rowid) FROM term_normalised;\")\n",
    "row1 = cur1.fetchone()\n",
    "total = row1[0]\n",
    "if total == None: total = 0\n",
    "for i in range(1, total+1):\n",
    "    tokens = []\n",
    "    cur2.execute(\"SELECT token FROM term_bag WHERE id = ? ORDER BY token;\"\"\", (i,))\n",
    "    rows2 = cur2.fetchall()\n",
    "    for row2 in rows2: tokens.append(row2[0])\n",
    "    expanded = \" \".join(tokens)\n",
    "    cur2.execute(\"UPDATE term_normalised SET expanded = ? WHERE rowid = ?;\", (expanded, i))\n",
    "        \n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Tokens normalised in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- identify nested MWTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- speed up searching through the phrases\n",
    "cur1.execute(\"CREATE INDEX idx08 ON term_phrase(normalised);\")\n",
    "cur1.execute(\"CREATE INDEX idx09 ON term_normalised(normalised);\")\n",
    "cur1.execute(\"CREATE INDEX idx10 ON term_normalised(expanded);\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "cur1.execute(\"DELETE FROM term_nested_aux;\")\n",
    "cur1.execute(\"DELETE FROM term_nested;\")\n",
    "###\n",
    "\n",
    "# --- select candidate MWT pairs to check for nestedness\n",
    "for i in range(1, total):\n",
    "\n",
    "    # --- progress bar\n",
    "    sys.stdout.write('\\r')\n",
    "    p = int(100*i/total)+1\n",
    "    sys.stdout.write(\"[%-100s] %d%%\" % ('='*p, p))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    cur1.execute(\"SELECT token FROM term_bag WHERE id = ?;\", (i,))\n",
    "    tokens = \"\"\n",
    "    rows1 = cur1.fetchall()\n",
    "    for row1 in rows1: \n",
    "        tokens += \"','\" + row1[0].replace(\"'\", \"''\")\n",
    "    tokens = tokens[2:] + \"'\"\n",
    "   \n",
    "    cur1.execute(\"SELECT DISTINCT id FROM term_bag WHERE id > ? AND token IN (\"+tokens+\");\", (i,))\n",
    "    rows1 = cur1.fetchall()\n",
    "    for row1 in rows1:\n",
    "        \n",
    "        j = row1[0]\n",
    "        \n",
    "        # --- term_i - term_j = 0 ?\n",
    "        cur2.execute(\"\"\"SELECT token FROM term_bag WHERE id = ?\n",
    "                        EXCEPT\n",
    "                        SELECT token FROM term_bag WHERE id = ?;\"\"\", (i,j))\n",
    "        row2 = cur2.fetchone()\n",
    "        if row2 == None: # --- term_i subset of term_j\n",
    "            cur3.execute(\"INSERT INTO term_nested_aux(parent, child) VALUES(?,?)\", (j,i))\n",
    "        else:\n",
    "            # --- term_j - term_i = 0 ?\n",
    "            cur2.execute(\"\"\"SELECT token FROM term_bag WHERE id = ?\n",
    "                            EXCEPT\n",
    "                            SELECT token FROM term_bag WHERE id = ?;\"\"\", (j,i))\n",
    "            row2 = cur2.fetchone()\n",
    "            if row2 == None: # --- term_j subset of term_i\n",
    "                cur3.execute(\"INSERT INTO term_nested_aux(parent, child) VALUES(?,?)\", (i,j))\n",
    "\n",
    "# --- select unique nested MWT pairs\n",
    "cur1.execute(\"\"\"INSERT INTO term_nested(parent, child)\n",
    "                SELECT DISTINCT N1.expanded, N2.expanded\n",
    "                FROM   term_normalised N1, term_normalised N2, term_nested_aux A\n",
    "                WHERE  N1.rowid = A.parent\n",
    "                AND    N2.rowid = A.child\n",
    "                AND    N1.expanded <> N2.expanded;\"\"\") # --- proper subsets only\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"CREATE INDEX idx11 ON term_nested(parent);\")\n",
    "cur1.execute(\"CREATE INDEX idx12 ON term_nested(child);\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- calculate termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C-value (collocation)\n",
    "def cValue(length, f, s, nf):\n",
    "    c = f\n",
    "    if s > 0: c -= nf*1.0 / s\n",
    "    c = c * math.log(length)\n",
    "    return c\n",
    "\n",
    "# --- inverse document frequency (discriminativeness)\n",
    "def idf(n, df):\n",
    "    return math.log10(n*1.0 / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "cur1.execute(\"DELETE FROM term_termhood;\")\n",
    "cur1.execute(\"DELETE FROM term_output;\")\n",
    "###\n",
    "\n",
    "cur1.execute(\"\"\"INSERT INTO term_termhood(expanded, len, s, nf)\n",
    "                SELECT DISTINCT expanded, len, 0, 0 FROM term_normalised;\"\"\")\n",
    "\n",
    "# --- calculate frequency of standalone occurrence\n",
    "cur1.execute(\"\"\"SELECT N.expanded, COUNT(*)\n",
    "                FROM   term_normalised N, term_phrase P\n",
    "                WHERE  N.normalised = P.normalised\n",
    "                GROUP BY N.expanded;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    expanded = row1[0]\n",
    "    f        = row1[1]\n",
    "    cur2.execute(\"UPDATE term_termhood SET f = ? WHERE expanded = ?;\", (f, expanded))\n",
    "\n",
    "# --- calculate the number of parent (superset) MWTs\n",
    "cur1.execute(\"SELECT child, COUNT(*) FROM term_nested GROUP BY child;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    child = row1[0]\n",
    "    s     = row1[1]\n",
    "\n",
    "    cur2.execute(\"UPDATE term_termhood SET s = ? WHERE expanded = ?;\", (s, child))\n",
    "\n",
    "# --- calculate the frequency of nested occurrence\n",
    "cur1.execute(\"\"\"SELECT child, COUNT(*)\n",
    "                FROM   term_nested N, term_normalised C, term_phrase P\n",
    "                WHERE  N.parent = C.expanded\n",
    "                AND    C.normalised = P.normalised\n",
    "                GROUP BY child;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    child = row1[0]\n",
    "    nf    = row1[1]\n",
    "\n",
    "    cur2.execute(\"UPDATE term_termhood SET nf = ? WHERE expanded = ?;\", (nf, child))\n",
    "\n",
    "# --- add up frequencies (both nested and standalone): f(t)\n",
    "cur1.execute(\"UPDATE term_termhood SET f = f + nf;\")\n",
    "\n",
    "# --- calculate C-value\n",
    "cur1.execute(\"SELECT expanded, len, f, s, nf FROM term_termhood;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    expanded = row1[0]\n",
    "    length   = row1[1]\n",
    "    f        = row1[2]\n",
    "    s        = row1[3]\n",
    "    nf       = row1[4]\n",
    "    c        = cValue(length, f, s, nf) # --- NOTE: no ln(x) in sqlite, so have to calculate C-value externally\n",
    "    \n",
    "#    if c > 1:\n",
    "    cur2.execute(\"UPDATE term_termhood SET c = ? WHERE expanded = ?;\", (c, expanded))\n",
    "\n",
    "# --- store term list\n",
    "cur1.execute(\"\"\"INSERT INTO term_output(id, variant, c, f)\n",
    "                SELECT T.rowid, LOWER(P.phrase) as variant, T.c, COUNT(*)\n",
    "                FROM   term_termhood T, term_normalised N, term_phrase P\n",
    "                WHERE  T.expanded = N.expanded\n",
    "                AND    N.normalised = P.normalised\n",
    "                AND    T.f > ?\n",
    "                AND    T.c > ?\n",
    "                GROUP BY T.rowid, variant, T.c;\"\"\", (Fmin, Cmin))\n",
    "\n",
    "# --- delete outliers: highly ranked terms that have a single variant with frequency of 1\n",
    "#     (e.g. kappa b), which is ranked highly only because of nested frequency\n",
    "cur1.execute(\"\"\"SELECT id FROM term_output O WHERE f <= ?\n",
    "                AND    1 = (SELECT COUNT(*) FROM term_output I WHERE O.id = I.id);\"\"\", (Fmin,))\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1: \n",
    "    cur2.execute(\"DELETE FROM term_output WHERE id = ?;\", (row1[0],))\n",
    "\n",
    "# --- n = total number of documents (to calculate IDF later on)\n",
    "cur1.execute(\"SELECT COUNT(*) FROM data_document;\")\n",
    "n = cur1.fetchone()[0]\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Termhood calculated in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- find term occurrences in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"DELETE FROM output_label;\")\n",
    "\n",
    "common = ['all', \n",
    "          'on', \n",
    "          'in', \n",
    "          'at', \n",
    "          'to', \n",
    "          'by', \n",
    "          'of', \n",
    "          'off', \n",
    "          'so', \n",
    "          'or', \n",
    "          'as', \n",
    "          'and', \n",
    "          'ie',\n",
    "          'eg',\n",
    "          'dr',\n",
    "          'mr',\n",
    "          'mrs',\n",
    "          'ms',\n",
    "          'km',\n",
    "          'mm',\n",
    "          'old',\n",
    "          'no', \n",
    "          'not', \n",
    "          'pre',\n",
    "          'be', \n",
    "          'is', \n",
    "          'are', \n",
    "          'am', \n",
    "          'can', \n",
    "          'for', \n",
    "          'up',\n",
    "          'has',\n",
    "          'had',\n",
    "          'who']\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "print(\"Retrieving terms to match...\")\n",
    "cur1.execute(\"SELECT id, variant FROM term_output;\")\n",
    "rows1 = cur1.fetchall()\n",
    "total = len(rows1)\n",
    "i = 0\n",
    "for row1 in rows1:\n",
    "\n",
    "    matcher.add(str(row1[0]), None, nlp(row1[1]))\n",
    "    \n",
    "    # --- progress bar\n",
    "    i += 1\n",
    "    sys.stdout.write('\\r')\n",
    "    p = int(100*i/total)\n",
    "    sys.stdout.write(\"[%-100s] %d%%\" % ('='*p, p))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\nLooking up terms in documents...\")\n",
    "cur1.execute(\"SELECT id, document FROM data_document;\")\n",
    "rows1 = cur1.fetchall()\n",
    "total = len(rows1)\n",
    "i = 0\n",
    "for row1 in rows1:\n",
    "    doc_id= row1[0]\n",
    "    doc = nlp(hyphen(row1[1]))\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # --- progress bar\n",
    "    i += 1\n",
    "    sys.stdout.write('\\r')\n",
    "    p = int(100*i/total)\n",
    "    sys.stdout.write(\"[%-100s] %d%%\" % ('='*p, p))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        term_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end].text\n",
    "        o = len(span)\n",
    "        s = len(doc[0:end].text) - o\n",
    "        if (span.lower() not in common) or span.upper() == span: # --- making sure that short acronyms such as OR are uppercased to avoid FPs\n",
    "            cur2.execute(\"INSERT INTO output_label(doc_id, start, offset, label) VALUES (?,?,?,?);\", (doc_id, s, o, term_id))\n",
    "\n",
    "# --- update document frequency\n",
    "cur1.execute(\"\"\"SELECT label, COUNT(DISTINCT doc_id) as df\n",
    "                FROM   output_label\n",
    "                GROUP BY label;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    label = row1[0]\n",
    "    df    = row1[1]\n",
    "    cur2.execute(\"UPDATE term_output SET df=?, idf=? WHERE id = ?;\", (df, idf(n,df), label))\n",
    "\n",
    "# --- delete nested labels\n",
    "cur1.execute(\"\"\"DELETE FROM output_label WHERE rowid IN (\n",
    "                SELECT T2.rowid FROM output_label T1, output_label T2\n",
    "                WHERE  T1.doc_id = T2.doc_id\n",
    "                AND    T1.start <= T2.start\n",
    "                AND    T2.start + T2.offset <= T1.start + T1.offset\n",
    "                AND    (T1.start != T2.start OR T2.start + T2.offset != T1.start + T1.offset));\"\"\")\n",
    "\n",
    "# --- delete overlapping labels\n",
    "cur1.execute(\"\"\"DELETE FROM output_label WHERE rowid IN (\n",
    "                SELECT T2.rowid FROM output_label T1, output_label T2\n",
    "                WHERE  T1.doc_id = T2.doc_id\n",
    "                AND    T2.start <= T1.start + T1.offset\n",
    "                AND    T1.start + T1.offset <= T2.start + T2.offset\n",
    "                AND    (T1.start != T2.start OR T2.start + T2.offset != T1.start + T1.offset));\"\"\")\n",
    "\n",
    "# --- delete terms that have no occurrences (it may happen \n",
    "#     when they are nested in a term, which was mistagged)\n",
    "cur1.execute(\"DELETE FROM term_output WHERE id NOT IN (SELECT label FROM output_label);\")\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Term occurrences annotated in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1.execute(\"CREATE INDEX idx13 ON term_output(id);\")\n",
    "cur1.execute(\"CREATE INDEX idx14 ON term_output(c, id, f);\")\n",
    "cur1.execute(\"CREATE INDEX idx15 ON output_label(doc_id);\")\n",
    "cur1.execute(\"CREATE INDEX idx16 ON output_label(label);\")\n",
    "cur1.execute(\"CREATE INDEX idx17 ON output_label(label, doc_id);\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- annotate term occurrences in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- color scaling\n",
    "def transition(value, maximum, start_point, end_point):\n",
    "    return start_point + (end_point - start_point)*value/maximum\n",
    "\n",
    "def transition3(value, maximum):\n",
    "    r1= transition(value, maximum, 37, 211)\n",
    "    r2= transition(value, maximum, 150, 234)\n",
    "    r3= transition(value, maximum, 190, 242)\n",
    "    return \"#%02x%02x%02x\" % (int(r1), int(r2), int(r3))\n",
    "\n",
    "# --- random color picking\n",
    "def color_generator(number_of_colors):\n",
    "    color = [\"#\"+''.join([random.choice('9ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n",
    "    return color\n",
    "\n",
    "# --- top C-value score\n",
    "cur1.execute(\"SELECT MAX(c) FROM term_output;\")\n",
    "top = cur1.fetchone()[0]\n",
    "\n",
    "random_colors = True\n",
    "\n",
    "# --- asign colors to terms\n",
    "entities = []\n",
    "colors = {\"ENT\":\"#E8DAEF\"}\n",
    "\n",
    "cur1.execute(\"SELECT DISTINCT id, c FROM term_output ORDER BY c DESC;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    id = row1[0]\n",
    "    c  = row1[1]\n",
    "    entities.append(str(id))\n",
    "    color = transition3(top-c, top)\n",
    "    colors[str(id)] = color\n",
    "\n",
    "if random_colors:\n",
    "    color = color_generator(len(entities))\n",
    "    for i in range(len(entities)): colors[entities[i]] = color[i]\n",
    "\n",
    "# --- coloring options for spacy's PhraseMatcher\n",
    "options = {\"ents\": entities, \"colors\": colors}\n",
    "\n",
    "# --- spacy-formatted entity annotations\n",
    "annotations = []\n",
    "\n",
    "# --- for each document\n",
    "cur1.execute(\"SELECT id, document FROM data_document;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    doc_id = row1[0]\n",
    "    doc = row1[1]\n",
    "\n",
    "    # --- retrieve previously stored PhraseMatcher labels\n",
    "    ents = []\n",
    "    cur2.execute(\"SELECT start, offset, label FROM output_label WHERE doc_id = ?;\", (doc_id,))\n",
    "    rows2 = cur2.fetchall()\n",
    "    for row2 in rows2:\n",
    "        start = row2[0]\n",
    "        end = row2[0] + row2[1]\n",
    "        label = str(row2[2])\n",
    "        ents.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "\n",
    "    annotations.append({\"text\": doc, \"ents\": ents, \"title\": doc_id, \"settings\": {}})\n",
    "\n",
    "# --- export spacy-formatted entity annotations\n",
    "with open(Path(\"./out/annotations.json\"), \"w\") as file:\n",
    "    json.dump(annotations, file, indent=4)\n",
    "    file.close()\n",
    "\n",
    "# --- visualise annotations\n",
    "html = displacy.render(annotations, style=\"ent\", manual=True, options=options, page=True, jupyter=False)\n",
    "html = re.sub('>([^<]+)</h2>', ' id=\"D\\\\1\">\\\\1</h2>', html, flags=re.IGNORECASE)\n",
    "\n",
    "# --- export HTML visualisation/annotation\n",
    "with open(Path(\"./out/corpus.html\"), \"w\", encoding=\"utf8\") as file: \n",
    "    file.write(html)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header(title):\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<title>\"\"\" + title + \"\"\"</title>\n",
    "<style></style>\n",
    "</head>\n",
    "<body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- extract concordances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance(id, doc_id, left, term, right): \n",
    "    return \"\"\"\n",
    "    <tr>\n",
    "        <td><a href='corpus.html#D\"\"\"+ doc_id +\"\"\"' target='_blank'>\"\"\" + doc_id + \"\"\"</a></td>\n",
    "        <td style='text-align:right'>\"\"\" + left + \"\"\"</td>\n",
    "        <td style='text-align:center;width:1px;white-space:nowrap;'>\n",
    "        <mark class=\"entity\" style=\"background: \"\"\" + colors[str(id)] + \"\"\"; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\"\"\" + term + \"\"\"</mark>\n",
    "        </td>\n",
    "        <td>\"\"\" + right  + \"\"\"</td>\n",
    "    </tr>\"\"\"\n",
    "\n",
    "# --- start an HTML document\n",
    "output = header(\"Concordances\")\n",
    "output += \"<h1>Term concordances</h1>\"\n",
    "\n",
    "cur1.execute(\"SELECT DISTINCT id FROM term_output ORDER BY c DESC;\")\n",
    "rows1 = cur1.fetchall()\n",
    "for row1 in rows1:\n",
    "    id = row1[0]\n",
    "\n",
    "    output += \"\\n<br/><br/>\\n<h2 style='margin:0' id='T\" + str(id) + \"'>Term ID: <a href='terminology.html#L\"+ str(id) +\"' target='_blank'>\"+ str(id) +\"</a></h2><br/><table border='0'>\"\n",
    "    \n",
    "    cur2.execute(\"\"\"SELECT doc_id,\n",
    "                           SUBSTR(D.document, MAX(start+1-80, 1), MIN(start, 80)), \n",
    "                           SUBSTR(D.document, start+1, offset),\n",
    "                           SUBSTR(D.document, start+1+offset, 80)\n",
    "                    FROM   output_label L, data_document D\n",
    "                    WHERE  label = ?\n",
    "                    AND    L.doc_id = D.id\n",
    "                    ORDER BY doc_id, start;\"\"\", (id,))\n",
    "    rows2 = cur2.fetchall()\n",
    "    for row2 in rows2: output += concordance(id, row2[0], row2[1], row2[2], row2[3])\n",
    "    \n",
    "    # --- close the table\n",
    "    output += \"\\n</table>\"\n",
    "\n",
    "# --- end the HTML document\n",
    "output += \"\\n</html>\"\n",
    "\n",
    "# --- write HTML to file\n",
    "with open(Path(\"./out/concordances.html\"), \"w\", encoding=\"utf8\") as file: \n",
    "    file.write(output)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Concordances extracted in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- export terminology (lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- export terminology into a CSV file\n",
    "\n",
    "cur1.execute(\"\"\"SELECT id, variant, c, f, df, ROUND(c*idf, 3) AS c_idf\n",
    "                FROM   term_output\n",
    "                ORDER BY c DESC, id ASC, f DESC;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "   \n",
    "with open(Path(\"./out/terminology.csv\"), \"w\", encoding=\"utf8\") as file:\n",
    "    csv_writer = csv.writer(file, delimiter=\"\\t\")\n",
    "    csv_writer.writerow([i[0] for i in cur1.description])\n",
    "    csv_writer.writerows(rows1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- export terminology into an HTML file\n",
    "\n",
    "def firstrow(id, c, variant, f): \n",
    "    id = str(id)\n",
    "    c = str(round(c, 3))\n",
    "    f = str(f)\n",
    "    return \"\"\"\n",
    "    <tr>\n",
    "        <td rowspan='xxxxx' style='text-align:center'><a href='concordances.html#T\"\"\" + id + \"\"\"' target='_blank'>\"\"\" + id + \"\"\"</a></td>\n",
    "        <td rowspan='xxxxx' style='text-align:center'>\"\"\" + c  + \"\"\"</td>\n",
    "        <td id='L\"\"\"+ str(id) +\"\"\"' bgcolor='\"\"\" + colors[str(id)] + \"\"\"'>\"\"\" + variant + \"\"\"</td>\n",
    "        <td style='text-align:center'>\"\"\" + f + \"\"\"</td>\n",
    "    </tr>\"\"\"\n",
    "\n",
    "def nextrow(variant, f): \n",
    "    f = str(f)\n",
    "    return \"\"\"\n",
    "    <tr>\n",
    "        <td bgcolor='\"\"\" + colors[str(id)] + \"\"\"'>\"\"\" + variant + \"\"\"</td>\n",
    "        <td style='text-align:center'>\"\"\" + f + \"\"\"</td>\n",
    "    </tr>\"\"\"\n",
    "\n",
    "# --- start an HTML document\n",
    "output = header(\"Terminology\")\n",
    "output += \"\"\"\n",
    "<h1>Terminology</h1>\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Term ID</th>\n",
    "        <th>Termhood</th>\n",
    "        <th>Term variant</th>\n",
    "        <th>Term variant frequency</th>\n",
    "    </tr>\"\"\"\n",
    "\n",
    "cur1.execute(\"\"\"SELECT id, variant, c, f\n",
    "                FROM   term_output\n",
    "                ORDER BY c DESC, id ASC, f DESC;\"\"\")\n",
    "rows1 = cur1.fetchall()\n",
    "pre = -1     # --- previous term ID\n",
    "tr = \"\"      # --- current table row\n",
    "rowspan = 0  # --- total of variants per ID\n",
    "for row1 in rows1:\n",
    "    id = row1[0]\n",
    "    if id != pre: # --- next term\n",
    "        output += tr.replace(\"xxxxx\", str(rowspan))\n",
    "        tr = firstrow(id, row1[2], row1[1], row1[3])\n",
    "        pre = id\n",
    "        rowspan = 1\n",
    "    else:         # --- append to the current term\n",
    "        tr += nextrow(row1[1], row1[3])\n",
    "        rowspan += 1\n",
    "\n",
    "# --- don't forget to add the last term ???\n",
    "if tr != \"\": output += tr.replace(\"xxxxx\", str(rowspan))\n",
    "\n",
    "# --- end the HTML document\n",
    "output += \"\\n</table>\\n</html>\"\n",
    "output = re.sub('<style></style>', '<style>td, th {border: 1px solid #999; padding: 0.5rem;}</style>', output, flags=re.IGNORECASE)\n",
    "\n",
    "# --- write HTML to file\n",
    "with open(Path(\"./out/terminology.html\"), \"w\", encoding=\"utf8\") as file: \n",
    "    file.write(output)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.perf_counter()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f\"Terminology exported in {run_time:0.4f} seconds\")\n",
    "timer.append(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- close the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "i = 0\n",
    "timer[2] += timer[5]\n",
    "for t in timer:\n",
    "    if i != 5: print(f\"{t:0.3f}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
